{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0096bf-dc7e-4e01-8513-7aca68745114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import AutoTokenizer, GPT2Model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28d7d25-024a-42fe-bfd8-36b72fd01f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.train_data = [\"../extracted_text/kumar_and_clark/kumar_and_clark_top_3.txt\",\"../extracted_text/Harrison/Harrison_top_3.txt\"]\n",
    "        self.test_data = \"\"\n",
    "        self.model_name = \"gpt2-xl\"\n",
    "        self.output_dir = f\"../model/qa_models/{self.model_name}-medicine/\"\n",
    "    \n",
    "def str_or_list(val):\n",
    "    if re.search(r\"^\\[\",val):\n",
    "        sep_list = val.strip(\"[]\").split(',')\n",
    "        return sep_list\n",
    "    return [val]\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--train_data\", help=\"Add input data files (single file name or list fo files in the format : [a,b,c,...]. The files in the list will be concatenated before being used as training data)\", required=True, type=str_or_list)\n",
    "# parser.add_argument(\"--test_data\", help=\"Add testing data files (single file name or list fo files in the format : [a,b,c,...]. The files in the list will be concatenated before being used as training data)\", required=False, type=str_or_list)\n",
    "# parser.add_argument(\"--model_name\", help=\"Model and tokenizer name\", required=True, type=str)\n",
    "# parser.add_argument(\"--output_dir\", help=\"Directory to save the trained models and checkpoints\", required=False, type=str, default=\"./\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = Args()\n",
    "# print(\"example :\")\n",
    "# print(dataset['train'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5915528b-8792-40b0-b2d1-4417e5baa6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../extracted_text/short_cases_medicine/short_cases_medicine_annotated.json\", 'r') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5698a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = json_data['respiratory']\n",
    "resp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b38dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"text\" : f\"{elem['med_hist']}\\n{elem['ques']}\", \"labels\" : elem['ans']} for elem in resp]\n",
    "# labels = [{\"text\" : elem['ans']} for elem in resp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd02f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train = Dataset.from_list(train)\n",
    "test = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849ce1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 129\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 33\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\"train\" : train, \"test\" : test})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32082af7-f34e-47f0-80c9-cda20024881d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c85c3d-6202-4038-b9ad-ce3e56a59772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "21ebf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "stride = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0c09b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    text_ids = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        # max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        # padding=True,\n",
    "        # stride=stride\n",
    "    )\n",
    "\n",
    "    label_ids = tokenizer(\n",
    "        element[\"label\"],\n",
    "        truncation=True,\n",
    "        # max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        # padding=True,\n",
    "        # stride=stride\n",
    "    )\n",
    "\n",
    "    text_batch = []\n",
    "    label_batch = []\n",
    "    for text_id, label_id in zip(text_ids[\"input_ids\"], label_ids[\"input_ids\"]):\n",
    "        # if length == context_length:\n",
    "        text_batch.append(text_id)\n",
    "        label_batch.append(label_id)\n",
    "    return {\"text_ids\": text_batch, \"label_ids\" : label_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1291861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd71d88223848d6a7812516cc96ab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10620ae1784c4f388ca2ea56ccb2f81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b99c434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4a52dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_ids', 'label_ids'],\n",
       "    num_rows: 129\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b55cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891f7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced1c309-b60d-4b35-9f6d-a97c61a5211d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_ids', 'label_ids'],\n",
      "        num_rows: 129\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_ids', 'label_ids'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(element):\n",
    "    text_ids = tokenizer(\n",
    "        element[\"text\"],\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    label_ids = tokenizer(\n",
    "        element[\"labels\"],\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    text_batch = []\n",
    "    label_batch = []\n",
    "    for text_id, label_id in zip(text_ids[\"input_ids\"], label_ids[\"input_ids\"]):\n",
    "        text_batch.append(text_id)\n",
    "        label_batch.append(label_id)\n",
    "\n",
    "    return {\"text_ids\": text_batch, \"label_ids\" : label_batch}\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b3a16-155c-4480-9f5f-12261450b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerate training loop\n",
    "\n",
    "def training_loop(args, dataset, mixed_precision=\"fp16\"):\n",
    "    \n",
    "    # model_name = \"bloom-1b1\"\n",
    "    model_name = args.model_name\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision = mixed_precision)\n",
    "    accelerator.print(\"accelerator initialised\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    accelerator.print(\"seed set\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"{model_name}\")\n",
    "    accelerator.print(\"model loaded\")\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset['train'], shuffle=False, batch_size=1)\n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    \n",
    "    num_epochs = 30\n",
    "    warm_up_steps = num_epochs//5 * len(train_dataloader)\n",
    "    training_steps = 4*num_epochs//5 * len(train_dataloader)\n",
    "    lr = 1e-5\n",
    "    checkpoint = True\n",
    "    load_checkpoint = False\n",
    "    evaluate = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    train_dataloader, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, model, optimizer\n",
    "    )\n",
    "    \n",
    "    if(args.test_data):\n",
    "        test_dataloader = DataLoader(dataset['test'], batch_size=1)\n",
    "        test_dataloader = accelerator.prepare(test_dataloader)\n",
    "        \n",
    "    accelerator.print(\"dataloaders initialised\")\n",
    "\n",
    "    accelerator.print(\"scheduler initialised\")\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer, num_warmup_steps=warm_up_steps, num_training_steps=training_steps\n",
    "    )\n",
    "    \n",
    "    # Training conditions    \n",
    "    \n",
    "    if load_checkpoint:\n",
    "        model.load_state_dict(torch.load(f'{args.output_dir}/{model_name}_medicine_epoch{epoch}.pth'))\n",
    "\n",
    "    progress_bar = tqdm(range(training_steps))\n",
    "    epoch_losses = []\n",
    "    best = 1\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss(reduction='mean')\n",
    "    model.train()\n",
    "    accelerator.print(\"training started\")\n",
    "    for epoch in range(num_epochs):\n",
    "        step_losses = []\n",
    "        for step,batch in enumerate(train_dataloader, start = 1):\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(batch['input_ids']).logits\n",
    "            labels = batch['labels']\n",
    "            # loss = causallm_loss(logits,batch)\n",
    "\n",
    "            preds = logits.view(-1, logits.size(-1))\n",
    "            targets = labels.view(-1)\n",
    "\n",
    "            loss = loss_fct(preds, targets)\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            step_losses.append([step,loss.item()])\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        epoch_loss = sum(step_losses)/len(step_losses)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        ppl = np.exp(epoch_loss)\n",
    "        with open(f\"{args.output_dir}resp_short_cases_finetune_logs.txt\",\"a\") as f:\n",
    "            f.write(f\"loss : {epoch_loss:.3f} , perplexity : + {ppl:.3f} \\n\")\n",
    "\n",
    "        if epoch_loss < epoch_losses[best-1]:\n",
    "            best = len(epoch_losses)\n",
    "        \n",
    "        if(checkpoint):\n",
    "            torch.save(model.state_dict(),f'{args.output_dir}/{model_name}_medicine_epoch{epoch}.pth')\n",
    "                \n",
    "    accelerator.print(\"training ended\")\n",
    "    accelerator.print(epoch_losses)\n",
    "    with open(\"../model/trained_models/logs.txt\",\"w\")as f:\n",
    "        f.write(f\"best = {best}\\n\" + str(epoch_losses))\n",
    "    # torch.save(model.state_dict(),f'../model/trained_models/{model_name}_harrison_respiratory.pth')\n",
    "    accelerator.print(\"best saved\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99939e74-7c7c-4a2d-a0c9-df45046b7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(args, tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059a37a-d8fb-4b7f-9961-6079522b2c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacda2b-ee9a-4fdc-ad7a-d0937f1d2d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd604f2-c495-48b9-bcd6-c0d544133d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b3329-49a7-42a6-9d9b-29e10a2eb92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
