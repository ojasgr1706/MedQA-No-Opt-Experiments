{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating inference on bloom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/bloom-3b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigscience/bloom-3b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    462\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m )\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:2230\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2230\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit:\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:791\u001b[0m, in \u001b[0;36mBloomForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: BloomConfig):\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mBloomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:596\u001b[0m, in \u001b[0;36mBloomModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings_layernorm \u001b[38;5;241m=\u001b[39m LayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;66;03m# Transformer blocks\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([BloomBlock(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Final Layer Norm\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m LayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:596\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings_layernorm \u001b[38;5;241m=\u001b[39m LayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;66;03m# Transformer blocks\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mBloomBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Final Layer Norm\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m LayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:413\u001b[0m, in \u001b[0;36mBloomBlock.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention \u001b[38;5;241m=\u001b[39m BloomAttention(config)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m LayerNorm(hidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mBloomMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_residual_connection_post_layernorm \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mapply_residual_connection_post_layernorm\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dropout \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_dropout\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:381\u001b[0m, in \u001b[0;36mBloomMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_h_to_4h \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_size)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu_impl \u001b[38;5;241m=\u001b[39m BloomGelu()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_4h_to_h \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dropout \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_dropout\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 25915,    632,    267, 144230,     34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is a tiger?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 25915,    632,    267, 144230,     34]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 25915,    632,    267, 144230,     34,    982,    603,   1502,   3548,\n",
      "           9313,    267,   9160,  10683,     15,    982,   7555,    368,  21380,\n",
      "             17,   1503]])\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs['input_ids'])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is a tiger?”\\n\\n“It’s a big cat,” said the boy. “']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (/u/ojasgramo/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b84c989952c4c7895029e3901ada3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/ojasgramo/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-8804bdd2bc004fb8.arrow\n",
      "Loading cached processed dataset at /u/ojasgramo/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-747c038866a29cbb.arrow\n"
     ]
    }
   ],
   "source": [
    "# bert tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer_bloom = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function_bloom(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets_bert = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/ojasgramo/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-d8b2e38283473510.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# bloom tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_bloom = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function_bloom(examples):\n",
    "    return tokenizer_bloom(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function_bloom, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets_bert['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m small_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_datasets\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m      2\u001b[0m small_eval_dataset \u001b[38;5;241m=\u001b[39m tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-3b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bigscience/bloom-3b\", num_labels=5)\n",
    "# model = model.to(\"cuda\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"../model/trained_model\", evaluation_strategy=\"epoch\", num_train_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preparing data (one time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../extracted_text/sample_train.txt\",\"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "with open(\"../extracted_text/sample_train.txt\",\"w\") as f:\n",
    "    f.write(text.replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../extracted_text/sample_test.txt\",\"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "with open(\"../extracted_text/sample_test.txt\",\"w\") as f:\n",
    "    f.write(text.replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"../extracted_text/Harrison/Harrison_text_top3.txt\",\"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "rat = int(np.floor(0.8*len(text)))\n",
    "train_text, test_text = text[:rat], text[rat:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../extracted_text/Harrison/train_data.txt\",\"w\") as f:\n",
    "    f.write(train_text)\n",
    "    \n",
    "with open(\"../extracted_text/Harrison/test_data.txt\",\"w\") as f:\n",
    "    f.write(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting-up dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/u/ojasgramo/.cache/huggingface/datasets/text/default-6ed7dd85d24ac69b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b508e8311aa54d3287d21966df542696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('text',data_files={'train': \"../extracted_text/sample/sample_train.txt\", 'test': \"../extracted_text/sample/sample_test.txt\"})\n",
    "print(dataset)\n",
    "# print(\"example :\")\n",
    "# print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Med Textbook data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/u/ojasgramo/.cache/huggingface/datasets/text/default-0dbdec9f8a09eed9/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34e6620e0d840feb3786a3f1580be9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1052\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 281\n",
      "    })\n",
      "})\n",
      "example :\n",
      "{'text': 'Edward T. Naureckas, Julian Solway '}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('text',data_files={'train': \"../extracted_text/Harrison/train_data.txt\", 'test': \"../extracted_text/Harrison/test_data.txt\"})\n",
    "print(dataset)\n",
    "print(\"example :\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Multidoc2dial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/dccstor/cgdial/ojasgramo/cache/huggingface/datasets/text/default-ad26ae56ab5a592f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda2076d9b324ed88dcc1919b790e908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 413\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 75\n",
      "    })\n",
      "})\n",
      "example :\n",
      "{'text': 'title : Retirement Estimator | Social Security Administration#1    Document :  Retirement Estimator  How the Retirement Estimator Works  The Retirement Estimator gives estimates based on your actual Social Security earnings record. Please keep in mind that these are just estimates. Estimate Your Retirement Benefits We can t provide your actual benefit amount until you apply for benefits. And that amount may differ from the estimates provided because : Your earnings may increase or decrease in the future. After you start receiving benefits , they will be adjusted for cost - of - living increases. Your estimated benefits are based on current law. The law governing benefit amounts may change because, by 2035, the combined trust fund reserves are projected to become depleted one year later than projected last year. Payroll taxes collected will be enough to pay only about 80 cents for each dollar of scheduled benefits. Your benefit amount may be affected by military service, railroad employment or pensions earned through work on which you did not pay Social Security tax.  Who Can Use the Retirement Estimator  You can use the Retirement Estimator if : You have enough Social Security credits at this time to qualify for benefits and You are not : Currently receiving benefits on your own Social Security record ; Waiting for a decision about your application for benefits or Medicare ; Age 62 or older and receiving benefits on another Social Security record ; or Eligible for a Pension Based on Work Not Covered By Social Security. If you are currently receiving only Medicare benefits , you can still get an estimate. For more information go to this link for our publication Retirement Information For Medicare Beneficiaries. If you cannot use the Retirement Estimator or you want a survivors or disability benefit estimate , please use one of our other benefit Calculators.  How Long Can You Stay On Each Page?  For security reasons , there are time limits for viewing each page. You will receive a warning after 25 minutes without doing anything , and you will be able to extend your time on the page. After the third warning on a page , you must move to another page. If you do not, your time will run out and your work on that page will be lost. '}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('text',data_files={'train': \"../extracted_text/multidoc2dial/title_doc_train.txt\", 'test': \"../extracted_text/multidoc2dial/title_doc_test.txt\"})\n",
    "print(dataset)\n",
    "print(\"example :\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "stride = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    dataset[\"train\"][:][\"text\"],\n",
    "    truncation=True,\n",
    "    # padding=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=stride,\n",
    "    return_length=True,\n",
    "    # padding=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n",
    "# print(f\"attention mask :\\n {outputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/413 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1376\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        padding=True,\n",
    "        stride=stride\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "            \n",
    "    # print(input_batch)\n",
    "\n",
    "    padded_batch = [stride*[tokenizer.pad_token_id] + input_batch[0][:stride]]\n",
    "    padded_batch += input_batch\n",
    "    # print(\"input_batch\")\n",
    "    # print(input_batch)\n",
    "    # print(\"padded_batch\")\n",
    "    # print(padded_batch)\n",
    "    # print(input_batch[0])\n",
    "    return {\"input_ids\": padded_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Setting-up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def causallm_loss(inputs, logits):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "    preds = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    targets = shift_labels.view(-1)\n",
    "    targets = targets.clone()\n",
    "    targets[:stride-1] = -100\n",
    "    \n",
    "\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "    loss = loss_fct(preds, targets)\n",
    "    # print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x148cbb7bb6a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 72378,  76754,  13748,   5123,    461,  49441,    396,  17552,  44392,\n",
      "          46737,    445,     17, 209711,   4543,  32955,     15,  89663,  11550,\n",
      "           5872,   1387,  25357,  16231,    461,    368, 145268,   5431,   2240,\n",
      "           1025,  58331,    655,  25266,    530,  94362,  27848, 122867,   2240,\n",
      "          33483,  16708,  13473,   5299,  25266,    530,  40943,   6738,     15,\n",
      "           2131,  85263,    262,  49885,    461, 145268,  57382,   5299,  25266,\n",
      "            530,   9119,     17,   3904,   4451,  39825,    361,    368,  51544,\n",
      "         203971,  17019,     15,   4618,  25266, 137096,   6147, 203971,  25178,\n",
      "          29826,   2222,    772,  16437,    632,  50363,   1485, 203971,  25178,\n",
      "           9119,   1331,    660,  47005,  53863,  56609,    461, 161320,    376,\n",
      "         207123,    530, 186975,  13953,     15,  19483,   2131, 145268]])}\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 11894,   4351,   2162,     37, 106163,  21576,   7011,  27379,   4100,\n",
       "          142143,   1541,   1306,    267,  97751,  25402,    461,    368,   8390,\n",
       "           21419,    461,   7011,  27379,     17,   4100, 142143,   1541,   1306,\n",
       "           55738,    322,  89908, 201582,  24595,    461, 145268,  71321,     15,\n",
       "           11762,  24279,  67454,   2022,  15323,     15, 197097,     15,  38281,\n",
       "          113563,     15,    530,     18,    280,   7458,    361,    368,  15966,\n",
       "             530,  12095,    461,   1999,    363,    440,     17,  12941,   2742,\n",
       "             791,   2742,   1130,    722,  85736,   1331,   3390,  54533,    461,\n",
       "           99775,     15,  11762, 104077,     15,   2670,  28617,   1779,     15,\n",
       "             530,  63439, 149630,     17,   1387, 180761,  10546, 227900,    461,\n",
       "          149042,   1541,    632,    267,  21419,    461,    267,  14951, 149042,\n",
       "             529]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem = next(it)\n",
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = elem['input_ids']\n",
    "inputs = inputs.to(device)\n",
    "logits = model(inputs).logits\n",
    "# loss = causallm_loss(inputs,logits)\n",
    "logits = logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11894,   4351,   2162,     37, 106163,  21576,   7011,  27379,   4100,\n",
       "         142143,   1541,   1306,    267,  97751,  25402,    461,    368,   8390,\n",
       "          21419,    461,   7011,  27379,     17,   4100, 142143,   1541,   1306,\n",
       "          55738,    322,  89908, 201582,  24595,    461, 145268,  71321,     15,\n",
       "          11762,  24279,  67454,   2022,  15323,     15, 197097,     15,  38281,\n",
       "         113563,     15,    530,     18,    280,   7458,    361,    368,  15966,\n",
       "            530,  12095,    461,   1999,    363,    440,     17,  12941,   2742,\n",
       "            791,   2742,   1130,    722,  85736,   1331,   3390,  54533,    461,\n",
       "          99775,     15,  11762, 104077,     15,   2670,  28617,   1779,     15,\n",
       "            530,  63439, 149630,     17,   1387, 180761,  10546, 227900,    461,\n",
       "         149042,   1541,    632,    267,  21419,    461,    267,  14951, 149042,\n",
       "            529]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4351,   2162,     37, 106163,  21576,   7011,  27379,   4100, 142143,\n",
       "           1541,   1306,    267,  97751,  25402,    461,    368,   8390,  21419,\n",
       "            461,   7011,  27379,     17,   4100, 142143,   1541,   1306,  55738,\n",
       "            322,  89908, 201582,  24595,    461, 145268,  71321,     15,  11762,\n",
       "          24279,  67454,   2022,  15323,     15, 197097,     15,  38281, 113563,\n",
       "             15,    530,     18,    280,   7458,    361,    368,  15966,    530,\n",
       "          12095,    461,   1999,    363,    440,     17,  12941,   2742,    791,\n",
       "           2742,   1130,    722,  85736,   1331,   3390,  54533,    461,  99775,\n",
       "             15,  11762, 104077,     15,   2670,  28617,   1779,     15,    530,\n",
       "          63439, 149630,     17,   1387, 180761,  10546, 227900,    461, 149042,\n",
       "           1541,    632,    267,  21419,    461,    267,  14951, 149042,    529]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[...,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1324, -2.4718,  6.0218,  ..., -2.3309, -2.3309, -2.3310],\n",
       "         [-1.9499, -2.7147,  3.8685,  ..., -2.0518, -2.0515, -2.0517],\n",
       "         [-2.3274, -2.8437,  5.7214,  ..., -2.4403, -2.4397, -2.4397],\n",
       "         ...,\n",
       "         [-1.5610, -1.6484,  4.1014,  ..., -1.7611, -1.7608, -1.7608],\n",
       "         [-1.5903, -2.2754,  5.3868,  ..., -1.8332, -1.8329, -1.8329],\n",
       "         [ 0.1187,  0.3553,  2.9793,  ...,  0.0454,  0.0453,  0.0454]]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[..., :-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_labels = inputs[..., 1:].contiguous()\n",
    "shift_logits = logits[..., :-1, :].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11894,   4351,   2162,     37, 106163,  21576,   7011,  27379,   4100,\n",
       "         142143,   1541,   1306,    267,  97751,  25402,    461,    368,   8390,\n",
       "          21419,    461,   7011,  27379,     17,   4100, 142143,   1541,   1306,\n",
       "          55738,    322,  89908, 201582,  24595,    461, 145268,  71321,     15,\n",
       "          11762,  24279,  67454,   2022,  15323,     15, 197097,     15,  38281,\n",
       "         113563,     15,    530,     18,    280,   7458,    361,    368,  15966,\n",
       "            530,  12095,    461,   1999,    363,    440,     17,  12941,   2742,\n",
       "            791,   2742,   1130,    722,  85736,   1331,   3390,  54533,    461,\n",
       "          99775,     15,  11762, 104077,     15,   2670,  28617,   1779,     15,\n",
       "            530,  63439, 149630,     17,   1387, 180761,  10546, 227900,    461,\n",
       "         149042,   1541,    632,    267,  21419,    461,    267,  14951, 149042,\n",
       "            529]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 250880])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4351,   2162,     37, 106163,  21576,   7011,  27379,   4100, 142143,\n",
       "           1541,   1306,    267,  97751,  25402,    461,    368,   8390,  21419,\n",
       "            461,   7011,  27379,     17,   4100, 142143,   1541,   1306,  55738,\n",
       "            322,  89908, 201582,  24595,    461, 145268,  71321,     15,  11762,\n",
       "          24279,  67454,   2022,  15323,     15, 197097,     15,  38281, 113563,\n",
       "             15,    530,     18,    280,   7458,    361,    368,  15966,    530,\n",
       "          12095,    461,   1999,    363,    440,     17,  12941,   2742,    791,\n",
       "           2742,   1130,    722,  85736,   1331,   3390,  54533,    461,  99775,\n",
       "             15,  11762, 104077,     15,   2670,  28617,   1779,     15,    530,\n",
       "          63439, 149630,     17,   1387, 180761,  10546, 227900,    461, 149042,\n",
       "           1541,    632,    267,  21419,    461,    267,  14951, 149042,    529]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = shift_logits.view(-1, shift_logits.size(-1))\n",
    "targets = shift_labels.view(-1)\n",
    "targets[:stride-1] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28.6269,  29.0666,  34.2686,  ...,  18.2569,  18.2567,  18.2531],\n",
       "        [316.8571, 322.1927, 330.4165,  ..., 191.7483, 191.7484, 191.7424],\n",
       "        [335.4773, 340.5488, 348.1035,  ..., 201.8398, 201.8396, 201.8339],\n",
       "        ...,\n",
       "        [396.2276, 398.3240, 411.2265,  ..., 205.2723, 205.2722, 205.2675],\n",
       "        [403.6878, 405.6578, 419.6461,  ..., 205.8794, 205.8795, 205.8745],\n",
       "        [393.0273, 395.1537, 412.2231,  ..., 203.2219, 203.2215, 203.2169]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "          -100,   -100,   -100,   -100,  46777,    488,    769,   6738,   8875,\n",
       "            17,  53148,   2194,     15,    613,    368,  51544,    427,    722,\n",
       "          6084,  35173,    361,    683, 198485,   9119,     15,    368,  40943,\n",
       "          9119, 191462,    461,    267,   8885, 203971, 217020,   6591,    722,\n",
       "         93740,    427,   3776,  21361,  12725,     17,  12298,     15,    661,\n",
       "           267,  75162,    461,  60596,    361,  45683, 119623,    530,   2730],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 7.3755e+00, 1.1336e-01, 2.3923e+00, 4.8047e+00, 1.4806e+00,\n",
       "        4.7544e+00, 7.3098e-03, 7.0966e-01, 2.3397e-02, 8.1592e-03, 1.1834e+00,\n",
       "        2.5144e+00, 2.2242e+00, 4.4376e+00, 2.5823e-03, 1.9017e-03, 7.0492e-01,\n",
       "        1.0569e+00, 1.5093e-02, 1.9259e+00, 2.9703e+00, 1.4238e+00, 2.5263e+00,\n",
       "        1.1498e+00, 2.3886e+00, 7.7971e-01, 2.5469e-01, 4.7980e+00, 1.6305e+00,\n",
       "        3.6897e-02, 5.0698e-02, 1.6914e+00, 6.9757e+00, 3.7764e-02, 3.2049e-01,\n",
       "        2.5938e+00, 7.5455e+00, 7.9338e+00, 6.9640e-01, 7.8515e-02, 7.0205e-01,\n",
       "        4.1812e-01, 3.8445e-01, 3.0616e+00, 1.1295e+00, 1.4363e-02, 3.5373e+00,\n",
       "        1.1990e+00, 1.2310e-01, 2.1295e-02], device='cuda:0',\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fct(preds,targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(291.8333, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_beg = loss\n",
    "loss_beg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(92.2099, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60596,    361,  45683, 119623,    530,   2730,    573,    525,  10393,\n",
       "           4657, 117826,    661,   6355,    661,    368,  22886,    461,  80396,\n",
       "             15,    261,  25559,  22754, 129673,     15,    530,  83076,    322,\n",
       "          62920,   1485,    368, 110350,  29826,     15,    368, 203971,  17019,\n",
       "          47030,    361,   3808,  20122,  72053,   1541,    530,  21361,  86452,\n",
       "           6582,    361,  13923,     17,   5070,    368, 145268,   5431,    427,\n",
       "          53103,    361,  58331,   4128,  25266,    530, 196208,   7011,     21,\n",
       "             15,    718,   6591,    722,  11045,    427,  72053,    655,    368,\n",
       "          51544, 158454,   2194,    530,  20495,    427,  42442,   4323, 203971,\n",
       "          25178,   9119,     30,    718,   6591,  13842,    613,  21361,  12725,\n",
       "            461,    368,  11559, 203971, 217020,    361,    267,  33365]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3112,  3.1807, 11.7497,  ...,  2.7595,  2.7589,  2.7596],\n",
       "         [ 0.7225,  2.5615, 10.3917,  ...,  1.7751,  1.7744,  1.7742],\n",
       "         [ 0.4262,  1.4538, 10.9861,  ...,  1.8934,  1.8941,  1.8931],\n",
       "         ...,\n",
       "         [-1.0838,  0.2338, 10.2983,  ...,  1.8735,  1.8717,  1.8732],\n",
       "         [-1.4234, -0.3629, 10.2829,  ...,  1.4542,  1.4536,  1.4548],\n",
       "         [-0.5848,  0.1232, 12.1440,  ...,  1.9798,  1.9792,  1.9798]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3112,  3.1807, 11.7497,  ...,  2.7595,  2.7589,  2.7596],\n",
       "         [ 0.7225,  2.5615, 10.3917,  ...,  1.7751,  1.7744,  1.7742],\n",
       "         [ 0.4262,  1.4538, 10.9861,  ...,  1.8934,  1.8941,  1.8931],\n",
       "         ...,\n",
       "         [ 0.2009,  1.9065, 11.9122,  ...,  1.7456,  1.7432,  1.7440],\n",
       "         [-1.0838,  0.2338, 10.2983,  ...,  1.8735,  1.8717,  1.8732],\n",
       "         [-1.4234, -0.3629, 10.2829,  ...,  1.4542,  1.4536,  1.4548]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   361,  45683, 119623,    530,   2730,    573,    525,  10393,   4657,\n",
       "         117826,    661,   6355,    661,    368,  22886,    461,  80396,     15,\n",
       "            261,  25559,  22754, 129673,     15,    530,  83076,    322,  62920,\n",
       "           1485,    368, 110350,  29826,     15,    368, 203971,  17019,  47030,\n",
       "            361,   3808,  20122,  72053,   1541,    530,  21361,  86452,   6582,\n",
       "            361,  13923,     17,   5070,    368, 145268,   5431,    427,  53103,\n",
       "            361,  58331,   4128,  25266,    530, 196208,   7011,     21,     15,\n",
       "            718,   6591,    722,  11045,    427,  72053,    655,    368,  51544,\n",
       "         158454,   2194,    530,  20495,    427,  42442,   4323, 203971,  25178,\n",
       "           9119,     30,    718,   6591,  13842,    613,  21361,  12725,    461,\n",
       "            368,  11559, 203971, 217020,    361,    267,  33365]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = shift_logits.view(-1, shift_logits.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = shift_labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 250880])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97, 250880])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cgdial/ojasgramo/anaconda3/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "loss_fct = CrossEntropyLoss(reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fct(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator('fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, test_dataloader, model, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training conditions\n",
    "\n",
    "checkpoint_steps = 300\n",
    "checkpoint = True\n",
    "load_checkpoint = False\n",
    "evaluate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_checkpoint:\n",
    "    model.load_state_dict(torch.load('../model/trained_models/bloom-560m_harrison_respiratory.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for step,batch in enumerate(train_dataloader, start = 1):\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(batch['input_ids']).logits\n",
    "        loss = causallm_loss(batch['input_ids'],logits)\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        losses.append([step,loss.item()])\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if(checkpoint & step%checkpoint_steps == 0):\n",
    "            torch.save(model.state_dict(),'../model/trained_models/bloom-560m_harrison_respiratory_{}.pth'.format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../model/trained_models/bloom-560m_harrison_respiratory.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerate training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def causallm_loss(inputs, logits):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "    preds = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    targets = shift_labels.view(-1)\n",
    "    targets = targets.clone()\n",
    "    targets[:stride-1] = -100\n",
    "    \n",
    "\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "    loss = loss_fct(preds, targets)\n",
    "    # print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bloom-1b1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"bigscience/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf30cf01b40445180a5d730cd269460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "model = GPT2Model.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_losses = []\n",
    "epoch_losses = []\n",
    "best = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(mixed_precision=\"fp16\"):\n",
    "    \n",
    "    model_name = \"bloom-1b1\"\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision = mixed_precision)\n",
    "    accelerator.print(\"accelerator initialised\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    accelerator.print(\"seed set\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"bigscience/{model_name}\")\n",
    "    accelerator.print(\"model loaded\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "    accelerator.print(\"dataloaders initialised\")\n",
    "    \n",
    "    train_dataloader, test_dataloader, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, test_dataloader, model, optimizer\n",
    "    )\n",
    "    \n",
    "    num_epochs = 10\n",
    "    warm_up_steps = num_epochs//5 * len(train_dataloader)\n",
    "    training_steps = 4*num_epochs//5 * len(train_dataloader)\n",
    "\n",
    "    accelerator.print(\"scheduler initialised\")\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer, num_warmup_steps=warm_up_steps, num_training_steps=training_steps\n",
    "    )\n",
    "    \n",
    "    # Training conditions\n",
    "\n",
    "    checkpoint = True\n",
    "    load_checkpoint = False\n",
    "    evaluate = False\n",
    "    \n",
    "    if load_checkpoint:\n",
    "        model.load_state_dict(torch.load(f'../model/trained_models/{model_name}_multidoc2dial_epoch{epoch}.pth'))\n",
    "\n",
    "    progress_bar = tqdm(range(training_steps))\n",
    "    step_losses = []\n",
    "    epoch_losses = []\n",
    "    best = 1\n",
    "    \n",
    "    model.train()\n",
    "    accelerator.print(\"training started\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for step,batch in enumerate(train_dataloader, start = 1):\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(batch['input_ids']).logits\n",
    "            loss = causallm_loss(batch['input_ids'],logits)\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            step_losses.append([step,loss.item()])\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        epoch_losses.append(sum(step_losses[-1]))\n",
    "        if epoch_losses[-1] < epoch_losses[best-1]:\n",
    "            best = len(epoch_losses)\n",
    "        \n",
    "        if(checkpoint):\n",
    "            torch.save(model.state_dict(),f'../model/trained_models/{model_name}_multidoc2dial_epoch{epoch+1}.pth')        \n",
    "                \n",
    "    accelerator.print(\"training ended\")\n",
    "    with open(\"../model/trained_models/logs.txt\",\"w\")as f:\n",
    "        f.write(f\"best = {best}\\n\" + epoch_losses)\n",
    "    # torch.save(model.state_dict(),f'../model/trained_models/{model_name}_harrison_respiratory.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "accelerator initialised\n",
      "seed set\n",
      "model loaded\n",
      "dataloaders initialised\n",
      "scheduler initialised\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39c41cd6f5346adb5c256c7bf297405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(training_loop, num_processes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(batch['input_ids'])\n",
    "    # outputs = model(**batch)\n",
    "    # print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../model/trained_models/bloom-3b_sample-data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7807)\n",
      "tensor(3.4755)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"bleu\")\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "        print(outputs.loss.sum())\n",
    "        \n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "        print(outputs)\n",
    "\n",
    "    # losses.append(accelerator.gather(outputs.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model = torch.load(\"../model/trained_models/bloom-560m_sample-data.pth\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to make lungs efficient in gas exchange\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7572,    427,   5219, 193711,  35173,    361,   9119,  40688,     15,\n",
       "            718,   6591,    722,  11045,    427,  72053,    655,    368,  51544,\n",
       "            361,    267,  33365,    861,  85263,    262,    368,  49885,    461,\n",
       "          57382,   5299,    368,  51544,    530,    368,   6738,     17,   3904,\n",
       "           4451,  39825,    361,    368,  51544,  29826,     15,   4618,    368,\n",
       "         145268,   5431,    632,  71941,    461,    267,  16852,    461,  53863,\n",
       "             15,  53863,     15,  53863,     15,  53863,     15,  53863,     15,\n",
       "          53863,     15,  53863,     15,  53863,     15,  53863,     15,  53863,\n",
       "             15,  53863,     15,  53863,     15,  53863,     15,  53863,     15,\n",
       "          53863,     15,  53863,     15,  53863,     15,  53863,     15,  53863,\n",
       "             15,  53863,     15,  53863,     15,  53863,     15,  53863,     15,\n",
       "          53863,     15,  53863,     15,  53863,     15,  53863,     15,  53863]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to make lungs efficient in gas exchange, it must be able to ventilate the lung in a manner that facilitates the diffusion of gases between the lung and the air. This process occurs in the lung wall, where the respiratory system is composed of a series of thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin, thin']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5])\n",
    "b = a.clone()\n",
    "b[2] = 34\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġis', 'Ġa', 'Ġ', 'custom_token1', 'Ġexample', 'Ġsentence', '.']\n",
      "['This', 'Ġis', 'Ġa', 'Ġ', 'custom_token2', 'Ġexample', 'Ġsentence', '.']\n",
      "['This', 'Ġis', 'Ġa', 'Ġ', 'custom_token3', 'Ġexample', 'Ġsentence', '.']\n",
      "['This', 'Ġis', 'Ġa', 'Ġ', 'custom_token4', 'Ġexample', 'Ġsentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "tokenizer.add_tokens([\"custom_token1\", \"custom_token2\"], special_tokens=True)\n",
    "tokenizer.add_tokens([\"custom_token3\", \"custom_token4\"], special_tokens=False)\n",
    "\n",
    "print(tokenizer.tokenize(\"This is a custom_token1 example sentence.\"))\n",
    "print(tokenizer.tokenize(\"This is a custom_token2 example sentence.\"))\n",
    "print(tokenizer.tokenize(\"This is a custom_token3 example sentence.\"))\n",
    "print(tokenizer.tokenize(\"This is a custom_token4 example sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "# tokenizer.add_tokens([\"custom_token1\", \"custom_token2\"], special_tokens=True)\n",
    "# tokenizer.add_tokens([\"custom_token3\", \"custom_token4\"], special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ' is', ' a', ' custom', '_', 'token', '1', ' example', ' sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(tokenizer(\"This is a custom_token1 example sentence.\")['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"custom_token3\", \"custom_token4\"], special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', ' is', ' a', ' ', '', ' example', ' sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(tokenizer(\"This is a custom_token3 example sentence.\")['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [250680], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('custom_token3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_special_tokens_mask(tokenizer('custom_token3')['input_ids'],already_has_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
