{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe949216-c9be-48a7-9679-1477fedc05a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/dccstor/cgdial/ojasgramo/cache/huggingface/datasets/text/default-9d77d126da39b7b3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e8998f5dc74947b8a068198b4e8f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2528\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "example :\n",
      "{'text': 'CHAPTER CONTENTS '}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('text',data_files={'train': [\"../extracted_text/kumar_and_clark/kumar_and_clark_top_1.txt\",\"../extracted_text/kumar_and_clark/kumar_and_clark_top_2.txt\"], 'test': \"../extracted_text/kumar_and_clark/kumar_and_clark_top_3.txt\"})\n",
    "print(dataset)\n",
    "print(\"example :\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e1878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c2c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae11a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e8ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c277a1-b0fd-46d5-ba22-51b5716ed381",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "stride = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961ef555-52cb-4eaf-a5f1-87126425fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53983303-ea6b-4e8d-b586-25feb8226968",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    dataset[\"train\"][:][\"text\"],\n",
    "    truncation=True,\n",
    "    # padding=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=stride,\n",
    "    return_length=True,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "# print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n",
    "# print(f\"attention mask :\\n {outputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70feee5f-aab3-4dbb-bf0e-31edf650f397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /dccstor/cgdial/ojasgramo/cache/huggingface/datasets/text/default-ad26ae56ab5a592f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f5516df2c58a8e9e.arrow\n",
      "Loading cached processed dataset at /dccstor/cgdial/ojasgramo/cache/huggingface/datasets/text/default-ad26ae56ab5a592f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1d3fe60dad49f47e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1376\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        padding=True,\n",
    "        stride=stride\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "            \n",
    "    # print(input_batch)\n",
    "\n",
    "    # padded_batch = [stride*[tokenizer.pad_token_id] + input_batch[0][:stride]]\n",
    "    # padded_batch += input_batch\n",
    "    # print(\"input_batch\")\n",
    "    # print(input_batch)\n",
    "    # print(\"padded_batch\")\n",
    "    # print(padded_batch)\n",
    "    # print(input_batch[0])\n",
    "    return {\"input_ids\": padded_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30967b97-df39-4e4b-afe8-996bd6cefaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9488b58d-bd22-48a9-a37f-e56cb53f25ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 1376\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7232aab-eb48-4801-9eeb-7c978b41468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db7ca0-9fc7-4592-a304-bdca3e6a8dc6",
   "metadata": {},
   "source": [
    "### Accelerate training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431272cf-9f79-44a2-af71-c6139a020682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c5e9fae-03df-497d-adae-9e395cf9d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1fdbbd0-fd75-48f6-9aed-0cd70428e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d60a2782-0d21-4f0b-ba02-bcd54a90604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def causallm_loss(inputs, logits):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "    preds = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    targets = shift_labels.view(-1)\n",
    "    targets = targets.clone()\n",
    "    targets[:stride-1] = -100\n",
    "    \n",
    "\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduction='sum')\n",
    "    loss = loss_fct(preds, targets)\n",
    "    # print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81f831c-1be3-4802-9d0f-ce2c0f79d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc80935b-4339-4324-8672-df85afa84f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4c01c14-a201-4d68-b27f-128119664d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_losses = []\n",
    "epoch_losses = []\n",
    "best = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18434ca-2371-43b5-828c-fa47f12598a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(mixed_precision=\"fp16\"):\n",
    "    \n",
    "    model_name = \"bloom-1b1\"\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision = mixed_precision)\n",
    "    accelerator.print(\"accelerator initialised\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    accelerator.print(\"seed set\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"bigscience/{model_name}\")\n",
    "    accelerator.print(\"model loaded\")\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "    accelerator.print(\"dataloaders initialised\")\n",
    "    \n",
    "    train_dataloader, test_dataloader, model, optimizer = accelerator.prepare(\n",
    "        train_dataloader, test_dataloader, model, optimizer\n",
    "    )\n",
    "    \n",
    "    num_epochs = 10\n",
    "    warm_up_steps = num_epochs//5 * len(train_dataloader)\n",
    "    training_steps = 4*num_epochs//5 * len(train_dataloader)\n",
    "\n",
    "    accelerator.print(\"scheduler initialised\")\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer, num_warmup_steps=warm_up_steps, num_training_steps=training_steps\n",
    "    )\n",
    "    \n",
    "    # Training conditions\n",
    "\n",
    "    checkpoint = True\n",
    "    load_checkpoint = False\n",
    "    evaluate = False\n",
    "    \n",
    "    if load_checkpoint:\n",
    "        model.load_state_dict(torch.load(f'../model/trained_models/{model_name}_multidoc2dial_epoch{epoch}.pth'))\n",
    "\n",
    "    progress_bar = tqdm(range(training_steps))\n",
    "    step_losses = []\n",
    "    epoch_losses = []\n",
    "    best = 1\n",
    "    \n",
    "    model.train()\n",
    "    accelerator.print(\"training started\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for step,batch in enumerate(train_dataloader, start = 1):\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(batch['input_ids']).logits\n",
    "            loss = causallm_loss(batch['input_ids'],logits)\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            step_losses.append([step,loss.item()])\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        epoch_losses.append(sum(step_losses[-1]))\n",
    "        if epoch_losses[-1] < epoch_losses[best-1]:\n",
    "            best = len(epoch_losses)\n",
    "        \n",
    "        if(checkpoint):\n",
    "            torch.save(model.state_dict(),f'../model/trained_models/{model_name}_multidoc2dial_epoch{epoch+1}.pth')        \n",
    "                \n",
    "    accelerator.print(\"training ended\")\n",
    "    accelerator.print(epoch_losses)\n",
    "    with open(\"../model/trained_models/logs.txt\",\"w\")as f:\n",
    "        f.write(f\"best = {best}\\n\" + str(epoch_losses))\n",
    "    # torch.save(model.state_dict(),f'../model/trained_models/{model_name}_harrison_respiratory.pth')\n",
    "    accelerator.print(\"best saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10af53b8-30ab-424d-bf2d-04be12f9b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "accelerator initialised\n",
      "seed set\n",
      "model loaded\n",
      "dataloaders initialised\n",
      "scheduler initialised\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41bcd154ec04a00a37a9fcd5714e2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "training ended\n",
      "[1764.7094421386719, 1554.6121978759766, 1504.0621337890625, 1617.4124298095703, 1533.3802337646484, 1427.6160354614258, 1381.1657495498657, 1376.7595613598824, 1376.4812118709087, 1376.5097506046295]\n",
      "best saved\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(training_loop, num_processes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec0236-730b-44fb-b0e4-caa9160f1d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0641db43-fb79-4a39-9cf1-4b63f5ef87f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'v', 'b', 'd']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"[aa,v,b,d]\"\n",
    "a.strip(\"[]\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fecb4-6291-4dee-9da7-2c636489b7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d7d98-4736-426e-8c6d-26e5b51b3f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
