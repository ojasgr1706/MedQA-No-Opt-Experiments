We are trying to generate backward results using codex prompt and KJ (with options) prompt + llama2-7b chat model.
We are using the model for reward modelling and binary classification training.
We want to see if the model has enough knowledge in the first place to be able to answer such difficult questions