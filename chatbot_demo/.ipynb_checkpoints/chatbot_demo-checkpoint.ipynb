{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import streamlit as st\n",
    "import argparse\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from genai.credentials import Credentials\n",
    "from genai.model import Model\n",
    "from genai.schemas import GenerateParams\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"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\")\n",
    "\n",
    "api_key = \"pak-zLIXfwz3UmF2EIOrXr-T4YNw8s3A0MUCZ-5pmFezfFY\"\n",
    "api_url = \"https://bam-api.res.ibm.com/v1\"\n",
    "creds = Credentials(api_key,api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = GenerateParams(decoding_method=\"sample\", max_new_tokens=1024, temperature = 1, top_p = 1, top_k = 50, typical_p = 1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "greedy_params = GenerateParams(decoding_method=\"greedy\", max_new_tokens=1024, repetition_penalty=1.1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "# Instantiate a model proxy object to send your requests\n",
    "model = Model(\"meta-llama/Llama-2-7b-chat\", params=params, credentials=creds)\n",
    "greedy_model = Model(\"meta-llama/Llama-2-7b-chat\", params=greedy_params, credentials=creds)\n",
    "\n",
    "reasoning_delimiter = \"Answer: \"\n",
    "output_delimiter = \"Q:\"\n",
    "instruction = \"Use just the given patient history to answer the question. Do not assume any further information about the patient. Strictly Limit your response to 200 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_exists(new_op,old_ops):\n",
    "    new_op = new_op.strip(\". \").lower()\n",
    "    for old_op in old_ops:\n",
    "        old_op = old_op.strip(\". \").lower()\n",
    "        # print(f\"new:{new_op}\")\n",
    "        # print(f\"old:{old_op}\")\n",
    "        if (new_op in old_op or old_op in new_op):\n",
    "            return(1)\n",
    "    return(0)\n",
    "\n",
    "def create_options(instring, num_unique_ops = 4, options_generate_limit = 15):\n",
    "    unique_options = []\n",
    "    all_options = []\n",
    "    op_to_reason = {}\n",
    "\n",
    "    # inputs = tokenizer(instring, return_tensors=\"pt\")\n",
    "    # inputs = inputs.to(device)\n",
    "\n",
    "    print(\"Creating Options\")\n",
    "\n",
    "    while(len(all_options) < options_generate_limit and len(unique_options) < num_unique_ops):\n",
    "\n",
    "        if not len(all_options):\n",
    "            outputs = greedy_model.generate([instring])[0]\n",
    "            # outputs = model.generate(inputs.input_ids, max_new_tokens = 1024, repetition_penalty = 1.1)\n",
    "        else:\n",
    "            outputs = model.generate([instring])[0]\n",
    "            # outputs = model.generate(inputs.input_ids, max_new_tokens = 1024, do_sample = True, temperature = 1, repetition_penalty = 1.1)\n",
    "\n",
    "        text_output = outputs.generated_text.strip()\n",
    "        # text_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        # text_output = text_output.split(instring)[-1]\n",
    "        # text_output = text_output.strip()\n",
    "\n",
    "        if reasoning_delimiter in text_output:\n",
    "            op_reasoning, op_text  = text_output.split(reasoning_delimiter)\n",
    "            op_reasoning, op_text = op_reasoning.strip(), op_text.strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "            print(\"Option Created :\",op_text)\n",
    "\n",
    "            if not option_exists(op_text,unique_options):\n",
    "                print(\"New Option :\", op_text)\n",
    "                unique_options.append(op_text)\n",
    "                op_to_reason[op_text] = op_reasoning\n",
    "            else:\n",
    "                print(\"Option already exists.. Discarding..\")\n",
    "            \n",
    "        else:\n",
    "            op_text = \"<parsing error>\"\n",
    "            print(op_text)\n",
    "\n",
    "        all_options.append(op_text)\n",
    "        print(\"Existing Options :\", unique_options, \"\\n\")\n",
    "\n",
    "    print(\"Final Options :\", unique_options)\n",
    "\n",
    "    return [unique_options,op_to_reason]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose an algorithm to run 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: MedCodex + Verifier (F + RM)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User input here :  A 68-year-old male comes to the physician for evaluation of right flank pain. He has a history of diabetes and peripheral artery disease. His blood pressure is 160/90 mm Hg. Physical examination shows abdominal tenderness and right flank tenderness. An ultrasound shows dilation of the right ureter and renal pelvis.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is :\n",
      " A 68-year-old male comes to the physician for evaluation of right flank pain. He has a history of diabetes and peripheral artery disease. His blood pressure is 160/90 mm Hg. Physical examination shows abdominal tenderness and right flank tenderness. An ultrasound shows dilation of the right ureter and renal pelvis.\n"
     ]
    }
   ],
   "source": [
    "close = False\n",
    "\n",
    "# st.title(\"Choose the algorithm you want to run :\")\n",
    "option_ind = int(input(\"choose an algorithm to run\"))\n",
    "algorithms = [\"MedCodex - Greedy\", \"Codex - Greedy\", \"MedCodex + Codex (F+B)\", \"MedCodex + Verifier (F + RM)\"]\n",
    "algo = algorithms[option_ind]\n",
    "# Drop-down menu with four options\n",
    "# option = st.selectbox(\"Choose an option:\", [\"MedCodex - Greedy\", \"Codex - Greedy\", \"MedCodex + Codex (F+B)\", \"MedCodex + Verifier (F + RM)\"])\n",
    "\n",
    "# Display the selected optio\n",
    "# st.write(\"You selected:\", option)\n",
    "print(\"You selected:\", algo)\n",
    "\n",
    "# user_input = st.text_input(\"Enter input prompt\", \"default_value_goes_here\")\n",
    "user_input = input(\"User input here : \")\n",
    "print(\"The input is :\\n\",user_input)\n",
    "\n",
    "output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input is :\n",
      " A 39-year-old man presents to the emergency department because of progressively worsening chest pain and nausea that started at a local bar 30 minutes prior. The pain radiates to the epigastric area. He has a 5-year history of untreated hypertension. He has smoked 1 pack of cigarettes daily for the past 5 years and started abusing cocaine 2 weeks before his emergency room visit. The patient is diaphoretic and in marked distress. Describe the steps of management for this patient\n"
     ]
    }
   ],
   "source": [
    "if algo == \"MedCodex - Greedy\":\n",
    "    with open(\"./prompts/kj_instruct_2.txt\", 'r') as f:\n",
    "        few_shot_cot = f.read()\n",
    "\n",
    "    instruction = \"Use just the given patient history to answer the question. Do not assume any further information about the patient. Strictly Limit your response to 200 words.\"\n",
    "    instring = f'''{few_shot_cot}\\n\\n{instruction}\\nQ: {user_input}\\nA: Let's think step-by-step.'''\n",
    "\n",
    "elif algo == \"Codex - Greedy\":\n",
    "    with open(\"./prompts/codex_2.txt\", 'r') as f:\n",
    "        few_shot_cot = f.read()\n",
    "    \n",
    "    instring = f'''{few_shot_cot}\\n\\nQ: {user_input}\\nA: Let's think step-by-step.'''\n",
    "\n",
    "reasoning_delimiter = \"Answer: \"\n",
    "\n",
    "# submit = st.button(\"Submit input\")\n",
    "submit = True\n",
    "\n",
    "if submit:\n",
    "\n",
    "    # st.write(\"The input is :\\n\",user_input)\n",
    "\n",
    "    # inputs = tokenizer(instring, return_tensors=\"pt\")\n",
    "    # inputs = inputs.to(device)\n",
    "    \n",
    "    greedy_out = greedy_model.generate([instring])[0]\n",
    "    greedy_output = greedy_out.generated_text.strip()\n",
    "    # greedy_out = model.generate(inputs.input_ids, max_new_tokens = 1024, repetition_penalty = 1.1)\n",
    "    # greedy_out = greedy_out.split(instring)[-1]\n",
    "    # greedy_output = greedy_out.strip()\n",
    "\n",
    "    # if reasoning_delimiter in greedy_output:\n",
    "    #     greedy_ans = greedy_output.split(reasoning_delimiter)[1].strip()\n",
    "    # else:\n",
    "    #     greedy_ans = \"<parsing error>\"\n",
    "\n",
    "    output = greedy_output\n",
    "    # output = \"F output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18210e8a409a4f128c0e744ebb8bf056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/libcudart.so.11.0'), PosixPath('/dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "reward_model = AutoModelForSequenceClassification.from_pretrained('meta-llama/Llama-2-7b-chat-hf', num_labels = 1)\n",
    "reward_model.load_adapter(f\"/dccstor/ojasgr/scripts/approaches/reasoning_verifier/trl/examples/scripts/output_old/7b-chat/checkpoint-1900/\")\n",
    "reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "reward_model = reward_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Options\n",
      "Option Created : ordering a CT scan of the abdomen and pelvis.\n",
      "New Option : ordering a CT scan of the abdomen and pelvis.\n",
      "Existing Options : ['ordering a CT scan of the abdomen and pelvis.'] \n",
      "\n",
      "Option Created : Ordering a urinalysis and urine culture, and performing a plain abdominal X-ray or CT scan.\n",
      "New Option : Ordering a urinalysis and urine culture, and performing a plain abdominal X-ray or CT scan.\n",
      "Existing Options : ['ordering a CT scan of the abdomen and pelvis.', 'Ordering a urinalysis and urine culture, and performing a plain abdominal X-ray or CT scan.'] \n",
      "\n",
      "Option Created : CT angiogram.\n",
      "New Option : CT angiogram.\n",
      "Existing Options : ['ordering a CT scan of the abdomen and pelvis.', 'Ordering a urinalysis and urine culture, and performing a plain abdominal X-ray or CT scan.', 'CT angiogram.'] \n",
      "\n",
      "Option Created : Nephrolithiasis.\n",
      "New Option : Nephrolithiasis.\n",
      "Existing Options : ['ordering a CT scan of the abdomen and pelvis.', 'Ordering a urinalysis and urine culture, and performing a plain abdominal X-ray or CT scan.', 'CT angiogram.', 'Nephrolithiasis.'] \n",
      "\n",
      "Final Options : ['ordering a CT scan of the abdomen and pelvis.', 'Ordering a urinalysis and urine culture, and performing a plain abdominal X-ray or CT scan.', 'CT angiogram.', 'Nephrolithiasis.']\n"
     ]
    }
   ],
   "source": [
    "if algo == \"MedCodex + Codex (F+B)\" or algo == \"MedCodex + Verifier (F + RM)\":\n",
    "\n",
    "    with open(\"./prompts/kj_instruct_2.txt\", 'r') as f:\n",
    "        few_shot_cot = f.read()\n",
    "\n",
    "    with open(\"./prompts/codex_2.txt\", 'r') as f:\n",
    "        backward_prompt = f.read()\n",
    "\n",
    "    instruction = \"Use just the given patient history to answer the question. Do not assume any further information about the patient. Strictly Limit your response to 200 words.\"\n",
    "    f_instring = f'''{few_shot_cot}\\n\\n{instruction}\\nQ: {user_input}\\nA: Let's think step-by-step.'''\n",
    "    \n",
    "    uniq_options, op_to_reason = create_options(f_instring)\n",
    "    if (len(uniq_options) < 4):\n",
    "        # output = \"Not enough options generated!\"\n",
    "        # st.write(\"Not enough Options\")\n",
    "        print(\"Not enough Options\")\n",
    "\n",
    "    else:\n",
    "        if algo == \"MedCodex + Codex (F+B)\":\n",
    "\n",
    "            options_text = \"\"\n",
    "            for op_num,op in enumerate(uniq_options):\n",
    "                options_text += f\"({chr(ord('A') + op_num)}) {op} \"\n",
    "\n",
    "            options_text = options_text.strip() + '\\n'\n",
    "\n",
    "            backward_ques = input(\"Input backward Question : \")\n",
    "            b_instring = f\"{backward_prompt}\\n\\nQ: {backward_ques}\\n{options_text}A: Let's think step-by-step.\"\n",
    "\n",
    "            # b_inputs = tokenizer(b_instring, return_tensors=\"pt\")\n",
    "            # b_inputs = b_inputs.to(device)\n",
    "\n",
    "            b_out = model.generate([b_instring])[0]\n",
    "            b_output = b_out.generated_text.strip()\n",
    "\n",
    "            # b_output = model.generate(b_inputs.input_ids, max_new_tokens = 1024, repetition_penalty = 1.1)\n",
    "            # b_output = b_output.split(b_instring)[-1]\n",
    "            # b_output = b_output.strip()\n",
    "\n",
    "            if b_output.strip() == \"\":\n",
    "                # st.write(\"output empty\")\n",
    "                print(\"output empty\")\n",
    "                b_output = \"<empty>\"\n",
    "            \n",
    "            # if str(elem[\"a\"]).strip() == \"\":\n",
    "            #     elem[\"a\"] = \"<empty>\"\n",
    "            \n",
    "            b_answer = re.findall(r\"\\([A-D]\\)\",b_output)\n",
    "            if len(b_answer):\n",
    "                b_answer = b_answer[0]\n",
    "                option = ord(b_answer[1]) - ord('A') + 1\n",
    "                b_answer_idx = ord(b_answer[1]) - ord('A')\n",
    "                output = b_answer + \" \" + op_to_reason[uniq_options[b_answer_idx]] + \"\\nAnswer: \" + uniq_options[b_answer_idx]\n",
    "            else:\n",
    "                b_answer =\"<parsing error>\" \n",
    "                option = 0\n",
    "                output = \"<parsing error>\"\n",
    "\n",
    "\n",
    "        elif algo == \"MedCodex + Verifier (F + RM)\":\n",
    "\n",
    "            instrings = []\n",
    "            logits = []\n",
    "\n",
    "            for k in range(4):\n",
    "                instring = f\"Question : {user_input}\\nReasoning : Let's think step by step. {op_to_reason[uniq_options[k]]}\\nAnswer : {uniq_options[k]}\"\n",
    "                \n",
    "            inputs = tokenizer(instring, padding = True, return_tensors=\"pt\")\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = reward_model(**inputs)\n",
    "\n",
    "            # logit = outputs.logits.item()\n",
    "            # logits.append(logit)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            logits = logits.reshape(-1)\n",
    "            logits = logits.tolist()\n",
    "\n",
    "            # print('logit', chr(ord('A') + k-1), f\"model : {logit:.3f}\")\n",
    "\n",
    "            answer_idx = np.argmax(logits)\n",
    "            answer = f\"({chr(answer_idx + ord('A'))})\"\n",
    "\n",
    "            output = answer + \" \" + op_to_reason[uniq_options[answer_idx]] + \"\\nAnswer: \" + uniq_options[answer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A) The patient has a history of diabetes and peripheral artery disease. The blood pressure is elevated which could indicate hypertension. The physical examination shows abdominal tenderness and right flank tenderness which could indicate kidney stones or infection. The ultrasound shows dilation of the right ureter and renal pelvis which confirms the diagnosis of kidney stones. The patient needs to undergo a CT scan of the abdomen and pelvis to confirm the diagnosis and rule out any complications such as obstruction or infection.\n",
      "Answer: ordering a CT scan of the abdomen and pelvis.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is te'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"this is te\\n\\n next line\"\n",
    "x.strip().split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rag]",
   "language": "python",
   "name": "conda-env-rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
