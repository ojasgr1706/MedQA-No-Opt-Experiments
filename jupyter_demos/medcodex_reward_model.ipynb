{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /dccstor/cgdial/ojasgramo/cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from genai.credentials import Credentials\n",
    "# from genai.model import Model\n",
    "# from genai.schemas import GenerateParams\n",
    "# from genai.extensions.langchain import LangChainInterface\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"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\")\n",
    "\n",
    "# api_key = \"pak-zLIXfwz3UmF2EIOrXr-T4YNw8s3A0MUCZ-5pmFezfFY\"\n",
    "# api_url = \"https://bam-api.res.ibm.com/v1\"\n",
    "# creds = Credentials(api_key,api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # params = GenerateParams(decoding_method=\"sample\", max_new_tokens=1024, temperature = 1, top_p = 1, top_k = 50, typical_p = 1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "    # greedy_params = GenerateParams(decoding_method=\"greedy\", max_new_tokens=1024, repetition_penalty=1.1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "    # # Instantiate a model proxy object to send your requests\n",
    "    # model = Model(\"meta-llama/Llama-2-7b-chat\", params=params, credentials=creds)\n",
    "    # greedy_model = Model(\"meta-llama/Llama-2-7b-chat\", params=greedy_params, credentials=creds)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained('meta-llama/Llama-2-7b-chat-hf', num_labels = 1)\n",
    "    reward_model.load_adapter(\"./reward_model/checkpoint-1900/\")\n",
    "    reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    reward_model = reward_model.to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return [model,reward_model,tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_exists(new_op,old_ops):\n",
    "    new_op = new_op.strip(\". \").lower()\n",
    "    for old_op in old_ops:\n",
    "        old_op = old_op.strip(\". \").lower()\n",
    "        # print(f\"new:{new_op}\")\n",
    "        # print(f\"old:{old_op}\")\n",
    "        if (new_op in old_op or old_op in new_op):\n",
    "            return(1)\n",
    "    return(0)\n",
    "\n",
    "def create_options(instring, num_unique_ops = 4, options_generate_limit = 15):\n",
    "    unique_options = []\n",
    "    all_options = []\n",
    "    op_to_reason = {}\n",
    "\n",
    "    inputs = tokenizer(instring, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    print(\"Creating Options\")\n",
    "\n",
    "    while(len(all_options) < options_generate_limit and len(unique_options) < num_unique_ops):\n",
    "\n",
    "        if not len(all_options):\n",
    "            # outputs = greedy_model.generate([instring])[0]\n",
    "            outputs = model.generate(inputs.input_ids, max_new_tokens = 1024, repetition_penalty = 1.1)\n",
    "        else:\n",
    "            # outputs = model.generate([instring])[0]\n",
    "            outputs = model.generate(inputs.input_ids, max_new_tokens = 1024, do_sample = True, temperature = 1, repetition_penalty = 1.1)\n",
    "\n",
    "        # text_output = outputs.generated_text.strip()\n",
    "        text_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        text_output = text_output.split(instring)[-1]\n",
    "        text_output = text_output.strip()\n",
    "\n",
    "        if reasoning_delimiter in text_output:\n",
    "            op_reasoning, op_text  = text_output.split(reasoning_delimiter)\n",
    "            op_reasoning, op_text = op_reasoning.strip(), op_text.strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "            print(\"Option Created :\",op_text)\n",
    "\n",
    "            if not option_exists(op_text,unique_options):\n",
    "                print(\"New Option Accepted\", op_text)\n",
    "                unique_options.append(op_text)\n",
    "                op_to_reason[op_text] = op_reasoning\n",
    "            else:\n",
    "                print(\"Option already exists.. Discarding..\")\n",
    "            \n",
    "        else:\n",
    "            op_text = \"<parsing error>\"\n",
    "            print(op_text)\n",
    "\n",
    "        all_options.append(op_text)\n",
    "\n",
    "    print(\"Final Options :\", unique_options)\n",
    "\n",
    "    return [unique_options,op_to_reason]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f9de81a5ea45cbbadc38a0ec235988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f8f1af57274b5ab24d8f07ec5fd113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/libcudart.so'), PosixPath('/dccstor/cgdial/ojasgramo/anaconda3/envs/rag/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model, reward_model, tokenizer = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/kj_instruct_2.txt\", 'r') as f:\n",
    "    few_shot_cot = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_delimiter = \"Answer: \"\n",
    "output_delimiter = \"Q:\"\n",
    "instruction = \"Use just the given patient history to answer the question. Do not assume any further information about the patient. Strictly Limit your response to 200 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your Question:  A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear. He received this first course of neoadjuvant chemotherapy 1 week ago. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. What kind of Adverse drug reaction is this and which beneficial effect of the drug that caused this patient's symptoms is most likely due to which of the following actions\n"
     ]
    }
   ],
   "source": [
    "question = input(\"enter your Question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Options\n",
      "Option Created : Ototoxicity.\n",
      "New Option Accepted Ototoxicity.\n",
      "<parsing error>\n",
      "<parsing error>\n",
      "<parsing error>\n",
      "Option Created : Adverse Drug Reaction caused by Neoadjuvant Chemotherapy.\n",
      "New Option Accepted Adverse Drug Reaction caused by Neoadjuvant Chemotherapy.\n",
      "Option Created : Beneficial effect of the drug that caused this patient's symptoms is the ability of the drug to shrink the tumor before surgery.\n",
      "New Option Accepted Beneficial effect of the drug that caused this patient's symptoms is the ability of the drug to shrink the tumor before surgery.\n",
      "Option Created : Methotrexate induced Tinnitus.\n",
      "New Option Accepted Methotrexate induced Tinnitus.\n",
      "Final Options : ['Ototoxicity.', 'Adverse Drug Reaction caused by Neoadjuvant Chemotherapy.', \"Beneficial effect of the drug that caused this patient's symptoms is the ability of the drug to shrink the tumor before surgery.\", 'Methotrexate induced Tinnitus.']\n"
     ]
    }
   ],
   "source": [
    "f_instring = f'''{few_shot_cot}\\n\\n{instruction}\\nQ: {question}\\nA: Let's think step-by-step.'''\n",
    "uniq_options, op_to_reason = create_options(f_instring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(uniq_options) < 4):\n",
    "    # output = \"Not enough options generated!\"\n",
    "    print(\"Not enough Options\")\n",
    "\n",
    "else:\n",
    "    instrings = []\n",
    "    logits = []\n",
    "\n",
    "    for k in range(4):\n",
    "        instring = f\"Question : {question}\\nReasoning : Let's think step by step. {op_to_reason[uniq_options[k]]}\\nAnswer : {uniq_options[k]}\"\n",
    "        instrings.append(instring)\n",
    "\n",
    "    inputs = tokenizer(instrings, padding = True, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(**inputs)\n",
    "\n",
    "    # logit = outputs.logits.item()\n",
    "    # logits.append(logit)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    logits = logits.reshape(-1)\n",
    "    logits = logits.tolist()\n",
    "\n",
    "    # print('logit', chr(ord('A') + k-1), f\"model : {logit:.3f}\")\n",
    "\n",
    "    answer_idx = np.argmax(logits)\n",
    "    answer = f\"({chr(answer_idx + ord('A'))})\"\n",
    "\n",
    "    output = \"Option \" + answer + \"\\n\" + op_to_reason[uniq_options[answer_idx]] + \"\\nAnswer: \" + uniq_options[answer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is: Option (A)\n",
      "Patient has a history of Transitional Cell Carcinoma of Bladder. He recently received Neoadjuvant Chemotherapy 1 week back. He now presents with a 2-day history of Ringing sensation in his Ear. Pure Tone Audiometry shows a sensorineural hearing loss of 45 dB. Neoadjuvant Chemotherapy used was most likely Cisplatin based. Cisplatin is known to cause Ototoxicity, which is the damage caused to the inner ear due to the use of certain chemotherapeutic drugs. Specifically, Cisplatin causes damage to the hair cells in the cochlea leading to permanent sensorineural hearing loss. The beneficial effect of the drug that caused this patient's symptoms is most likely due to the Ototoxicity caused by Cisplatin.\n",
      "Answer: Ototoxicity.\n"
     ]
    }
   ],
   "source": [
    "print(\"The output is:\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.416202068328857,\n",
       " 2.1811840534210205,\n",
       " 2.5864768028259277,\n",
       " -2.3294334411621094]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rag]",
   "language": "python",
   "name": "conda-env-rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
