{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /dccstor/cgdial/ojasgramo/cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from genai.credentials import Credentials\n",
    "# from genai.model import Model\n",
    "# from genai.schemas import GenerateParams\n",
    "# from genai.extensions.langchain import LangChainInterface\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"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\")\n",
    "\n",
    "# api_key = \"pak-zLIXfwz3UmF2EIOrXr-T4YNw8s3A0MUCZ-5pmFezfFY\"\n",
    "# api_url = \"https://bam-api.res.ibm.com/v1\"\n",
    "# creds = Credentials(api_key,api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # params = GenerateParams(decoding_method=\"sample\", max_new_tokens=1024, temperature = 1, top_p = 1, top_k = 50, typical_p = 1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "    # greedy_params = GenerateParams(decoding_method=\"greedy\", max_new_tokens=1024, repetition_penalty=1.1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "    # # Instantiate a model proxy object to send your requests\n",
    "    # model = Model(\"meta-llama/Llama-2-7b-chat\", params=params, credentials=creds)\n",
    "    # greedy_model = Model(\"meta-llama/Llama-2-7b-chat\", params=greedy_params, credentials=creds)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # reward_model = AutoModelForSequenceClassification.from_pretrained('meta-llama/Llama-2-7b-chat-hf', num_labels = 1)\n",
    "    # reward_model.load_adapter(\"./reward_model/checkpoint-1900/\")\n",
    "    # reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    # reward_model = reward_model.to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return [model,tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e6062a3d1440448d1e93aa82f5f7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/codex_2.txt\", 'r') as f:\n",
    "    few_shot_cot = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_delimiter = \"Answer: \"\n",
    "output_delimiter = \"Q:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your Question:  A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear. He received this first course of neoadjuvant chemotherapy 1 week ago. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. What kind of Adverse drug reaction is this and which beneficial effect of the drug that caused this patient's symptoms is most likely due to which of the following actions\n"
     ]
    }
   ],
   "source": [
    "question = input(\"enter your Question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instring = f'''{few_shot_cot}\\n\\nQ: {question}\\nA: Let's think step-by-step.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(instring, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# greedy_out = greedy_model.generate([instring])[0]\n",
    "# greedy_output = greedy_out.generated_text.strip()\n",
    "greedy_out = model.generate(inputs.input_ids, max_new_tokens = 1024, repetition_penalty = 1.1)\n",
    "greedy_out = tokenizer.batch_decode(greedy_out, skip_special_tokens=True)[0]\n",
    "greedy_out = greedy_out.split(instring)[-1]\n",
    "greedy_output = greedy_out.strip()\n",
    "\n",
    "if reasoning_delimiter in greedy_output:\n",
    "    greedy_ans = greedy_output.split(reasoning_delimiter)[1].strip()\n",
    "else:\n",
    "    greedy_output = \"<parsing error>\"\n",
    "    greedy_ans = \"<parsing error>\"\n",
    "\n",
    "# output = greedy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is: Ototoxicity caused by neoadjuvant chemotherapy.\n",
      "The Reasoning is as follows: The patient has a history of transitional cell carcinoma of the bladder and has just completed a course of neoadjuvant chemotherapy. He now presents with a 2-day history of ringing sensation in his ear. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. The adverse drug reaction is most likely ototoxicity, which is the damage caused to the inner ear by certain drugs. Chemotherapy drugs, especially cisplatin and other platinum-based drugs, are known to cause ototoxicity. The beneficial effect of the drug that caused this patient's symptoms is most likely the improvement in the cancer, as neoadjuvant chemotherapy is used to shrink the tumor before surgery or radiation therapy.\n",
      "Answer: Ototoxicity caused by neoadjuvant chemotherapy.\n"
     ]
    }
   ],
   "source": [
    "print(\"The output is:\",greedy_ans)\n",
    "print(\"The Reasoning is as follows:\",greedy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rag]",
   "language": "python",
   "name": "conda-env-rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
