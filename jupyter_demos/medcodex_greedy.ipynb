{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /dccstor/cgdial/ojasgramo/cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# from genai.credentials import Credentials\n",
    "# from genai.model import Model\n",
    "# from genai.schemas import GenerateParams\n",
    "# from genai.extensions.langchain import LangChainInterface\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = \"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\")\n",
    "\n",
    "# api_key = \"pak-zLIXfwz3UmF2EIOrXr-T4YNw8s3A0MUCZ-5pmFezfFY\"\n",
    "# api_url = \"https://bam-api.res.ibm.com/v1\"\n",
    "# creds = Credentials(api_key,api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # params = GenerateParams(decoding_method=\"sample\", max_new_tokens=1024, temperature = 1, top_p = 1, top_k = 50, typical_p = 1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "    # greedy_params = GenerateParams(decoding_method=\"greedy\", max_new_tokens=1024, repetition_penalty=1.1, stop_sequences=[\"\\nQ: \",\"Use just the given patient history to answer the question.\"], return_options={\"generated_tokens\" : True, \"token_logprobs\" : True})\n",
    "    # # Instantiate a model proxy object to send your requests\n",
    "    # model = Model(\"meta-llama/Llama-2-7b-chat\", params=params, credentials=creds)\n",
    "    # greedy_model = Model(\"meta-llama/Llama-2-7b-chat\", params=greedy_params, credentials=creds)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # reward_model = AutoModelForSequenceClassification.from_pretrained('meta-llama/Llama-2-7b-chat-hf', num_labels = 1)\n",
    "    # reward_model.load_adapter(\"./reward_model/checkpoint-1900/\")\n",
    "    # reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    # reward_model = reward_model.to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return [model,tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62160b6527a4d1f86ca7687e27a6c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompts/kj_instruct_2.txt\", 'r') as f:\n",
    "    few_shot_cot = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_delimiter = \"Answer: \"\n",
    "output_delimiter = \"Q:\"\n",
    "instruction = \"Use just the given patient history to answer the question. Do not assume any further information about the patient. Strictly Limit your response to 200 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter your Question:  A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Describe the appropriate ethical action that the resident must take in this condition\n"
     ]
    }
   ],
   "source": [
    "question = input(\"enter your Question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instring = f'''{few_shot_cot}\\n\\n{instruction}\\nQ: {question}\\nA: Let's think step-by-step.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(instring, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# greedy_out = greedy_model.generate([instring])[0]\n",
    "# greedy_output = greedy_out.generated_text.strip()\n",
    "greedy_out = model.generate(inputs.input_ids, max_new_tokens = 1024, repetition_penalty = 1.1)\n",
    "greedy_out = tokenizer.batch_decode(greedy_out, skip_special_tokens=True)[0]\n",
    "greedy_out = greedy_out.split(instring)[-1]\n",
    "greedy_output = greedy_out.strip()\n",
    "\n",
    "if reasoning_delimiter in greedy_output:\n",
    "    greedy_ans = greedy_output.split(reasoning_delimiter)[1].strip()\n",
    "else:\n",
    "    greedy_output = \"<parsing error>\"\n",
    "    greedy_ans = \"<parsing error>\"\n",
    "\n",
    "# output = greedy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is: including the complication in the operative report.\n",
      "The Reasoning is as follows: As a resident, I have inadvertently cut a flexor tendon during a carpal tunnel repair procedure under the supervision of an attending physician. The attending physician has instructed me not to report this minor complication in the operative report. However, as a responsible and ethical resident, I cannot ignore this incident as it involves a potential harm to the patient. Reporting this complication is crucial to maintain transparency and accuracy in the patient's medical record. It also helps to ensure that similar incidents can be prevented in future cases. Failure to report this complication could lead to a delay in diagnosis or treatment of the patient, which could result in adverse consequences for the patient's health. Therefore, I must include this complication in the operative report and inform the attending physician accordingly.\n",
      "\n",
      "Answer: including the complication in the operative report.\n"
     ]
    }
   ],
   "source": [
    "print(\"The output is:\",greedy_ans)\n",
    "print(\"The Reasoning is as follows:\",greedy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rag]",
   "language": "python",
   "name": "conda-env-rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
